{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Obesity prediction\n",
    "### Group 19\n",
    "- Ana Costa\n",
    "- Daniel Rodrigues\n",
    "- Pedro Moreira"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing üìä"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing plays a crucial role in unsupervised learning for several reasons:\n",
    "\n",
    "<strong>Data Cleaning:</strong>  üßπ Identifying and handling missing values, outliers, and noisy data ensures reliable and accurate input for the algorithm.\n",
    "\n",
    "<strong>Feature Scaling:</strong>  ‚öñÔ∏è Normalizing or standardizing features brings them to a similar scale, preventing dominance by features with larger ranges and ensuring equal weighting.\n",
    "\n",
    "<strong>Dimensionality Reduction:</strong>  üìè Reducing the number of features through techniques like PCA or t-SNE improves computational efficiency and facilitates visualization.\n",
    "\n",
    "H<strong>andling Categorical Variables:</strong>  üîÑ Transforming categorical variables into numerical representations, like one-hot encoding, enables compatibility with unsupervised algorithms.\n",
    "\n",
    "<strong>Outlier Detection:</strong> üö´ Detecting and addressing outliers using statistical or density-based methods improves accuracy and reliability.\n",
    "\n",
    "By performing data pre-processing, we enhance data quality, eliminate noise and inconsistencies, and enable accurate clustering, anomaly detection, and pattern discovery in unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T12:45:25.902031Z",
     "start_time": "2023-05-01T12:45:25.857211Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "obesity_data = pd.read_csv('data/obesity_dataset.csv')\n",
    "obesity_data_original_untouched = obesity_data.copy()\n",
    "target = 'NObeyesdad'\n",
    "\n",
    "obesity_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the data is important as it helps understand its structure and quality, assess data issues, select relevant features, visualize variables, and plan data preparation. It provides insights for decision-making, such as data cleaning and pre-processing. By exploring the data, patterns and relationships can be identified, facilitating effective analysis. Overall, previewing the data is a crucial step for gaining initial understanding, addressing quality issues, and planning subsequent data processing and analysis steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Checking for missing values </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Duplicate Data: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_data.duplicated().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we found there are duplicated lines we are now going to eliminate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_data = obesity_data.drop_duplicates(keep='first')\n",
    "obesity_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T12:45:27.203795Z",
     "start_time": "2023-05-01T12:45:27.134870Z"
    }
   },
   "outputs": [],
   "source": [
    "obesity_data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the data allows us to understand that there are no missing values since we have 2111 lines and exactly 2111 full cells for every one of our attributes. \n",
    "\n",
    "<strong>Measures of Central Tendency:</strong>\n",
    "\n",
    "The average value for each attribute is estimated using the mean value. For instance, the average age is roughly 24.31 years old, the average height is roughly 1.70 meters, and the average weight is roughly 86.59 kg.\n",
    "\n",
    "The center value in the dataset is known as the median. For instance, the average age is around 22.78 years old, the average height is about 1.70 meters, and the average weight is about 83.00 kg.\n",
    "\n",
    "<strong>Measures of Dispersion:</strong>\n",
    "\n",
    "The standard deviation (std) reflects how widely the data are dispersed from the mean. For instance, the standard deviations of age, height, and weight are each around 6.35 years, 0.09 meters, and 26.19 kg, respectively.\n",
    "\n",
    "Minimum (min): The minimum value denotes the lowest value that has been observed for a certain feature.\n",
    "\n",
    "Quartiles (25%): The value below which 25% of the data falls is represented by the first quartile's 25th percentile.\n",
    "\n",
    "Basic statistics of the dataset are summarized in the provided information, but more investigation and analysis are required to acquire a deeper understanding of the data and any underlying patterns or linkages.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Searching for outliers </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T12:45:59.888667Z",
     "start_time": "2023-05-01T12:45:27.980983Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# sb.pairplot(obesity_data, hue='NObeyesdad')\n",
    "\n",
    "numerical_columns = [\"Age\", \"Height\", \"Weight\"]\n",
    "\n",
    "num_columns = len(numerical_columns)\n",
    "num_rows = (num_columns + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(12, 8))\n",
    "\n",
    "for i, column in enumerate(numerical_columns):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax = axes[row, col]\n",
    "    sb.histplot(data=obesity_data, x=column, kde=True, ax=ax)\n",
    "    ax.set_title(f\"Histogram of {column}\")\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "# If the number of variables is odd, remove the empty subplot\n",
    "if num_columns % 2 != 0:\n",
    "    fig.delaxes(axes[num_rows-1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def scatter_plot():\n",
    "# Define the number of columns per row\n",
    "    columns_per_row = 3\n",
    "    numerical_columns = [\"Height\", \"Weight\", \"CH2O\", \"FAF\"]\n",
    "    # Calculate the total number of rows needed\n",
    "    total_plots = len(numerical_columns) * (len(numerical_columns) - 1) // 2\n",
    "    total_rows = (total_plots + columns_per_row - 1) // columns_per_row\n",
    "\n",
    "    # Create the overall figure and axis objects\n",
    "    fig, axs = plt.subplots(total_rows, columns_per_row, figsize=(15, 10))\n",
    "\n",
    "    # Flatten the axis objects to make indexing easier\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Iterate over the numerical columns combinations\n",
    "    plot_index = 0\n",
    "    for i in range(len(numerical_columns)):\n",
    "        for j in range(i + 1, len(numerical_columns)):\n",
    "            # Set the current axis for the plot\n",
    "            ax = axs[plot_index]\n",
    "\n",
    "            # Generate the scatter plot\n",
    "            sns.scatterplot(data=obesity_data, x=numerical_columns[i], y=numerical_columns[j], ax=ax)\n",
    "\n",
    "            # Set plot title, x-axis label, and y-axis label\n",
    "            ax.set_title(f\"Scatter Plot: {numerical_columns[i]} vs {numerical_columns[j]}\")\n",
    "            ax.set_xlabel(numerical_columns[i])\n",
    "            ax.set_ylabel(numerical_columns[j])\n",
    "\n",
    "            # Move to the next axis\n",
    "            plot_index += 1\n",
    "\n",
    "    # Remove empty subplots if the total number of plots is not a multiple of columns_per_row\n",
    "    if total_plots % columns_per_row != 0:\n",
    "        for j in range(total_plots % columns_per_row, columns_per_row):\n",
    "            fig.delaxes(axs[plot_index])\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()\n",
    "\n",
    "scatter_plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our histograms and scatter plots it's not very easy to spot and identify outliers so we shall use other methods like z-score.\n",
    "\n",
    "The z-score method is a statistical technique used to identify outliers in a dataset. It measures how many standard deviations a data point is away from the mean of the dataset. The z-score of a data point is calculated as:\n",
    "\n",
    "z = (x - Œº) / œÉ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantity of outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = obesity_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns of interest\n",
    "selected_columns = [\"Height\", \"Weight\", \"CH2O\", \"FAF\", \"NCP\", \"TUE\"]\n",
    "selected_data = obesity_data[selected_columns]\n",
    "\n",
    "# Calculate the z-scores for each column\n",
    "z_scores = np.abs(stats.zscore(selected_data))\n",
    "\n",
    "# Define the threshold for outliers\n",
    "threshold = 3\n",
    "\n",
    "# Find the indices of the outliers based on the z-scores\n",
    "outlier_indices = np.where(z_scores > threshold)\n",
    "\n",
    "# Print the indices and corresponding values of the outliers\n",
    "rows2remove = []\n",
    "for row, column in zip(*outlier_indices):\n",
    "    value = selected_data.iloc[row, column]\n",
    "    print(f\"Outlier found at index ({row}, {column}): {value}\")\n",
    "    rows2remove.append(row)\n",
    "\n",
    "\n",
    "obesity_data = obesity_data.drop(rows2remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init - obesity_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))  # Create a 1x3 grid for subplots\n",
    "\n",
    "sb.boxplot(x=obesity_data[\"Age\"], orient=\"h\", ax=ax[0])\n",
    "sb.boxplot(x=obesity_data[\"Height\"], orient=\"h\", ax=ax[1])\n",
    "sb.boxplot(x=obesity_data[\"Weight\"], orient=\"h\", ax=ax[2])\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=1.5, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the height and the weight we can find a few outliers. The code bellow gets ride of them so they don't disturbe our algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))  # Create a 1x3 grid for subplots\n",
    "\n",
    "sb.boxplot(x=obesity_data[\"NCP\"], orient=\"h\", ax=ax[0])\n",
    "sb.boxplot(x=obesity_data[\"FAF\"], orient=\"h\", ax=ax[1])\n",
    "sb.boxplot(x=obesity_data[\"TUE\"], orient=\"h\", ax=ax[2])\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=1.5, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def scatter_plot():\n",
    "# Define the number of columns per row\n",
    "    columns_per_row = 3\n",
    "    numerical_columns = [\"Height\", \"Weight\", \"CH2O\", \"FAF\"]\n",
    "    # Calculate the total number of rows needed\n",
    "    total_plots = len(numerical_columns) * (len(numerical_columns) - 1) // 2\n",
    "    total_rows = (total_plots + columns_per_row - 1) // columns_per_row\n",
    "\n",
    "    # Create the overall figure and axis objects\n",
    "    fig, axs = plt.subplots(total_rows, columns_per_row, figsize=(15, 10))\n",
    "\n",
    "    # Flatten the axis objects to make indexing easier\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Iterate over the numerical columns combinations\n",
    "    plot_index = 0\n",
    "    for i in range(len(numerical_columns)):\n",
    "        for j in range(i + 1, len(numerical_columns)):\n",
    "            # Set the current axis for the plot\n",
    "            ax = axs[plot_index]\n",
    "\n",
    "            # Generate the scatter plot\n",
    "            sns.scatterplot(data=obesity_data, x=numerical_columns[i], y=numerical_columns[j], ax=ax)\n",
    "\n",
    "            # Set plot title, x-axis label, and y-axis label\n",
    "            ax.set_title(f\"Scatter Plot: {numerical_columns[i]} vs {numerical_columns[j]}\")\n",
    "            ax.set_xlabel(numerical_columns[i])\n",
    "            ax.set_ylabel(numerical_columns[j])\n",
    "\n",
    "            # Move to the next axis\n",
    "            plot_index += 1\n",
    "\n",
    "    # Remove empty subplots if the total number of plots is not a multiple of columns_per_row\n",
    "    if total_plots % columns_per_row != 0:\n",
    "        for j in range(total_plots % columns_per_row, columns_per_row):\n",
    "            fig.delaxes(axs[plot_index])\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()\n",
    "\n",
    "scatter_plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first quadrant of the CH20 vs FAF, there are a few outliers. This is also true for the scatter plot Weight vs FAF, Weight vs CH2O and height vs FAF. They will all be removed in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows = set()\n",
    "\n",
    "for index, row in obesity_data.iterrows():\n",
    "    if row['Height'] > 1.96:\n",
    "        drop_rows.add(index)\n",
    "    if row['Weight'] > 159:\n",
    "        drop_rows.add(index)\n",
    "    if row['FAF'] > 1.97 and row['CH2O'] < 1.97:\n",
    "        drop_rows.add(index)\n",
    "    if row['Height'] < 1.47:\n",
    "        drop_rows.add(index)\n",
    "    if row['Weight'] > 135 and row['FAF'] < 0.4:\n",
    "        drop_rows.add(index)\n",
    "\n",
    "print(drop_rows)\n",
    "\n",
    "obesity_data = obesity_data.drop(drop_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def scatter_plot():\n",
    "# Define the number of columns per row\n",
    "    columns_per_row = 3\n",
    "    numerical_columns = [\"Height\", \"Weight\", \"CH2O\", \"FAF\"]\n",
    "    # Calculate the total number of rows needed\n",
    "    total_plots = len(numerical_columns) * (len(numerical_columns) - 1) // 2\n",
    "    total_rows = (total_plots + columns_per_row - 1) // columns_per_row\n",
    "\n",
    "    # Create the overall figure and axis objects\n",
    "    fig, axs = plt.subplots(total_rows, columns_per_row, figsize=(15, 10))\n",
    "\n",
    "    # Flatten the axis objects to make indexing easier\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Iterate over the numerical columns combinations\n",
    "    plot_index = 0\n",
    "    for i in range(len(numerical_columns)):\n",
    "        for j in range(i + 1, len(numerical_columns)):\n",
    "            # Set the current axis for the plot\n",
    "            ax = axs[plot_index]\n",
    "\n",
    "            # Generate the scatter plot\n",
    "            sns.scatterplot(data=obesity_data, x=numerical_columns[i], y=numerical_columns[j], ax=ax)\n",
    "\n",
    "            # Set plot title, x-axis label, and y-axis label\n",
    "            ax.set_title(f\"Scatter Plot: {numerical_columns[i]} vs {numerical_columns[j]}\")\n",
    "            ax.set_xlabel(numerical_columns[i])\n",
    "            ax.set_ylabel(numerical_columns[j])\n",
    "\n",
    "            # Move to the next axis\n",
    "            plot_index += 1\n",
    "\n",
    "    # Remove empty subplots if the total number of plots is not a multiple of columns_per_row\n",
    "    if total_plots % columns_per_row != 0:\n",
    "        for j in range(total_plots % columns_per_row, columns_per_row):\n",
    "            fig.delaxes(axs[plot_index])\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()\n",
    "\n",
    "scatter_plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering üí°üîß\n",
    "\n",
    "We've decided to aggregate the Weight and Height into a BMI attribute since it is much more representative and better fitting for what we are trying to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_data_feature_engineering = obesity_data\n",
    "# Criar a nova feature 'BMI_category'\n",
    "obesity_data_feature_engineering.insert(16, 'BMI', obesity_data['Weight'] / (obesity_data['Height'] ** 2))\n",
    "obesity_data_feature_engineering.drop(columns=['Height', 'Weight'], inplace=True)\n",
    "\n",
    "\n",
    "obesity_data_feature_engineering.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Data encoding</strong> will allow categorical variables to be represented numerically, ensuring compatibility with algorithms and preserving information. It prevents misinterpretations and enables distance-based calculations. üí° Encoding reduces memory usage and supports efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T12:45:59.888667Z",
     "start_time": "2023-05-01T12:45:30.715718Z"
    }
   },
   "outputs": [],
   "source": [
    "obesity_data_clean = obesity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T13:22:17.745176Z",
     "start_time": "2023-05-01T13:22:17.693914Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "for columnName in obesity_data_clean.columns:\n",
    "    if obesity_data_clean[columnName].dtype == \"float64\":\n",
    "        continue\n",
    "   \n",
    "    obesity_data_clean[columnName] = label_encoder.fit_transform(obesity_data_clean[columnName])\n",
    "    obesity_data_clean[columnName].unique()\n",
    "\n",
    "obesity_data_clean.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques address the curse of dimensionality. üìè Handling missing data and creating interpretable features enhance model robustness and explainability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# select the columns you want to scale\n",
    "cols_to_scale = ['Age', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE','BMI']\n",
    "\n",
    "# create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the selected columns\n",
    "scaler.fit(obesity_data_clean[cols_to_scale])\n",
    "\n",
    "# transform the selected columns\n",
    "obesity_data_clean[cols_to_scale] = scaler.transform(obesity_data_clean[cols_to_scale])\n",
    "\n",
    "obesity_data_clean = pd.DataFrame(obesity_data_clean, columns=[\n",
    "'Gender','Age','family_history_with_overweight','FAVC','FCVC','NCP','CAEC','SMOKE','CH2O','SCC','FAF','TUE','CALC','MTRANS','BMI','NObeyesdad'])\n",
    "\n",
    "obesity_data_clean.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing correlated inputs in data preprocessing is crucial for improving model performance by avoiding multicollinearity and overfitting. It eliminates redundant information, reduces dimensionality, and enhances interpretability of the model's results. By removing correlated inputs, we can ensure that the independence assumption holds, enabling machine learning algorithms to work optimally and provide accurate predictions. Overall, this step simplifies the data representation, promotes generalization, and helps the model make more meaningful and reliable predictions on unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use <strong>Pearson correlation</strong>  to quantify the linear relationship between two continuous variables. It ranges from -1 to 1, where a value close to 1 indicates a strong positive correlation, a value close to -1 indicates a strong negative correlation, and a value close to 0 suggests no significant correlation.\n",
    "\n",
    "It's important to note that Pearson correlation only captures linear relationships and may not detect non-linear associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr_matrix = obesity_data_clean.corr()\n",
    "\n",
    "# plot the correlation matrix\n",
    "import seaborn as sns\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "\n",
    "# create a list of highly correlated features to drop\n",
    "corr_features = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.65:\n",
    "            colname = corr_matrix.columns[i]\n",
    "            corr_features.add(colname)\n",
    "\n",
    "# drop the highly correlated features\n",
    "obesity_data_clean = obesity_data_clean.drop(corr_features, axis=1)\n",
    "obesity_data_clean.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest üå≥\n",
    "\n",
    "During training, multiple decision trees are constructed. In classification tasks, the class selected by the majority of trees becomes the output of the random forest. Random Forests address the issue of decision trees overfitting to the training set. They tend to outperform individual decision trees but may have lower accuracy compared to gradient boosted trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features (X) and target (y)\n",
    "X = obesity_data_clean.drop('NObeyesdad', axis=1)\n",
    "y = obesity_data_clean['NObeyesdad']\n",
    "\n",
    "# split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# create a Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# fit the Random Forest classifier to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# calculate feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# create a feature importance dataframe\n",
    "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
    "\n",
    "# sort the dataframe by feature importance\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "# plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(feature_importances['feature'], feature_importances['importance'])\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# select the top n features\n",
    "n = 10\n",
    "top_features = feature_importances['feature'][:n].tolist()\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this graph, several columns are unimportant for determining the degree of obesity. Therefore, we decided to create four different datasets to test the different performances of the implemented algorithms. From the complete dataset, columns with less importance were progressively removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset without SCC and SMOKE\n",
    "obesity_data_exp1 = obesity_data_clean[['Gender', 'Age', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'CH2O', 'FAF', 'TUE', 'CALC', 'MTRANS', 'BMI', 'NObeyesdad']]\n",
    "\n",
    "# create dataset without SCC, SMOKE, MTRANS, CAEC and FAVC\n",
    "obesity_data_exp2 = obesity_data_clean[['Gender', 'Age', 'family_history_with_overweight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'CALC', 'BMI', 'NObeyesdad']]\n",
    "\n",
    "# create dataset without SCC, SMOKE, MTRANS, CAEC, FAVC, family_history_with_overweight and CALC\n",
    "obesity_data_exp3 = obesity_data_clean[['Gender', 'Age', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'BMI', 'NObeyesdad']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datase with all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T12:45:59.892629Z",
     "start_time": "2023-05-01T12:45:59.888667Z"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = obesity_data_clean.iloc[:, :-1]\n",
    "y = obesity_data_clean.iloc[:, -1]\n",
    "\n",
    "def split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X0, X_test, y0, y_test = split(X, y)\n",
    "\n",
    "X0.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datase without SCC and SMOKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = obesity_data_exp1.iloc[:, :-1]\n",
    "y = obesity_data_exp1.iloc[:, -1]\n",
    "\n",
    "X1, X_test1, y1, y_test1 = split(X, y)\n",
    "\n",
    "X1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datase without SCC, SMOKE, MTRANS, CAEC and FAVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = obesity_data_exp2.iloc[:, :-1]\n",
    "y = obesity_data_exp2.iloc[:, -1]\n",
    "\n",
    "X2, X_test2, y2, y_test2 = split(X, y)\n",
    "\n",
    "X2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datase without SCC, SMOKE, MTRANS, CAEC, FAVC, family_history_with_overweight and CALC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = obesity_data_exp3.iloc[:, :-1]\n",
    "y = obesity_data_exp3.iloc[:, -1]\n",
    "\n",
    "X3, X_test3, y3, y_test3 = split(X, y)\n",
    "\n",
    "X3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing/Unbalancing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts = obesity_data_clean[target].value_counts()\n",
    "\n",
    "# Plotting the pie chart\n",
    "class_counts.plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Adding a title\n",
    "plt.title('Class Distribution')\n",
    "\n",
    "# Displaying the chart\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on the numbers for the class counts in our dataset, it appears that the classes are not heavily imbalanced. In this case, we decided to slightly improve the balance of our dataset, using random oversampling to adjust the class proportions.\n",
    "\n",
    "### Random Oversampling\n",
    "Randomly duplicate samples from the minority classes to match the count of the majority class. This can help increase the representation of the minority classes without significantly inflating the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separando features e target\n",
    "features = obesity_data_clean.drop('NObeyesdad', axis=1)\n",
    "target = obesity_data_clean['NObeyesdad']\n",
    "\n",
    "# Dividindo o conjunto de dados em treino e teste\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Random Oversampling\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = oversampler.fit_resample(X0, y0)\n",
    "X_train1, y_train1 = oversampler.fit_resample(X1, y1)\n",
    "X_train2, y_train2 = oversampler.fit_resample(X2, y2)\n",
    "X_train3, y_train3 = oversampler.fit_resample(X3, y3)\n",
    "\n",
    "\n",
    "# Plotting the class distribution before and after oversampling\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y0, palette='viridis')\n",
    "plt.title('Class Distribution (Before Oversampling)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_train, palette='viridis')\n",
    "plt.title('Class Distribution (After Oversampling)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search with Cross Validation\n",
    "\n",
    "Grid Search with Cross Validation optimizes hyperparameters by systematically searching for the best combination. It involves evaluating the model's performance for each hyperparameter combination using Cross Validation. By dividing the training data into subsets and iterating through the combinations, it finds the optimal hyperparameters. This technique prevents overfitting and provides a more generalized model. Grid Search with Cross Validation is a powerful method for hyperparameter tuning, ensuring the best model performance by exhaustively exploring the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "def cross_folding(model, space, n_folds = 10):\n",
    "    target_class = obesity_data_clean['NObeyesdad']\n",
    "    variables_features = obesity_data_clean.drop('NObeyesdad', axis=1)\n",
    "\n",
    "\n",
    "    cv_inner = KFold(n_splits=n_folds, shuffle=True, random_state=1)\n",
    "\n",
    "    search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv_inner, refit=True)\n",
    "\n",
    "    cv_outer = KFold(n_splits=n_folds, shuffle=True, random_state=1)\n",
    "\n",
    "    scores = cross_val_score(search, variables_features, target_class, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
    "    print('Accuracy: %.3f (%.3f)' % (max(scores), std(scores)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Decision Tree Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing accuracy of Decision Tree with diferent datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def decision_tree(X_train, y_train, X_test, y_test):\n",
    "    # make predictions on the test set\n",
    "   \n",
    "\n",
    "    # create a decision tree classifier object\n",
    "    dtc = DecisionTreeClassifier()\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    dtc.fit(X_train, y_train)\n",
    "\n",
    "    # visualize the decision tree\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plot_tree(dtc, feature_names=X_train.columns, class_names = ['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']\n",
    "    , filled=True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    y_pred = dtc.predict(X_test)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # calculate precision, recall, and F1-score for each class\n",
    "    precision = precision_score(y_test, y_pred, average=None)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "    # Make predictions for each class\n",
    "    y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
    "    # Calculate the predicted probabilities for each class\n",
    "    y_scores = dtc.predict_proba(X_test)\n",
    "\n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(np.unique(y))):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_scores[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label='Class {} (AUC = {:.2f})'.format(i, roc_auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # diagonal line for random classifier\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-Class ROC Curve (OvR)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # assume y_test is your true test set labels and y_pred is your predicted test set labels\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # plot confusion matrix heatmap\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy \n",
    "\n",
    "print(\"--------------------- DATASET WITH ALL COLUMNS --------------------------\\n\")\n",
    "decision_tree_accuracy = decision_tree(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE AND MTRANS -------------------------------------\")\n",
    "decision_tree_accuracy1 = decision_tree(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC AND FAVC -------------------------------------\")\n",
    "decision_tree_accuracy2 = decision_tree(X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC, FAVC, family_history_with_overweight AND CALC -------------------------------------\")\n",
    "decision_tree_accuracy3 = decision_tree(X_train3, y_train3, X_test3, y_test3)\n",
    "\n",
    "print(\"--------------------- Grid Search with cross validation --------------------------\\n\")\n",
    "\n",
    "space = dict()\n",
    "space['max_features'] = list(range(1, len(obesity_data_clean.drop('NObeyesdad', axis=1).columns)+1))\n",
    "cross_folding(DecisionTreeClassifier(), space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Nayve Bayes Algorithm\n",
    "\n",
    "The Naive Bayes algorithm is a simple probabilistic classifier based on Bayes' theorem. It assumes that features are conditionally independent given the class label. Despite its simplicity, Naive Bayes can perform well on many classification tasks, especially when there is a large amount of data available.\n",
    "\n",
    "However, Naive Bayes can be less effective on our datasets for the following reasons:\n",
    "\n",
    "Strong Feature Dependencies: The assumption of feature independence in Naive Bayes may not hold true if there are strong dependencies between features. In such cases, Naive Bayes can struggle to capture complex relationships and may result in suboptimal prediction\n",
    "\n",
    "Small data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def nayve_bayes(X_train, y_train, X_test, y_test):\n",
    "    gnb = GaussianNB()\n",
    "    # train the classifier on the training data\n",
    "    gnb.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = gnb.predict(X_test)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = gnb.score(X_test, y_test)\n",
    "    print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "\n",
    "    # create a confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # create a heatmap of the confusion matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Convert y_test and y_pred to binary format\n",
    "    y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "    y_pred_bin = label_binarize(y_pred, classes=np.unique(y_test))\n",
    "\n",
    "    # calculate precision and recall for each class\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin.ravel(), y_pred_bin.ravel())\n",
    "\n",
    "    # calculate micro-averaged precision and recall\n",
    "    micro_precision, micro_recall, _ = precision_recall_curve(y_test_bin.ravel(), y_pred_bin.ravel())\n",
    "\n",
    "    # plot the micro-averaged precision-recall curve\n",
    "    plt.plot(micro_recall, micro_precision, label='Micro-Averaged')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve - Dataset with All Columns')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy \n",
    "\n",
    "print(\"--------------------- DATASET WITH ALL COLUMNS --------------------------\\n\")\n",
    "nayve_bayes_accuracy = nayve_bayes(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE AND MTRANS -------------------------------------\")\n",
    "nayve_bayes_accuracy1 = nayve_bayes(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC AND FAVC -------------------------------------\")\n",
    "nayve_bayes_accuracy2 = nayve_bayes(X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC, FAVC, family_history_with_overweight AND CALC -------------------------------------\")\n",
    "nayve_bayes_accuracy3 = nayve_bayes(X_train3, y_train3, X_test3, y_test3)\n",
    "\n",
    "print(\"--------------------- Grid Search with cross validation --------------------------\\n\")\n",
    "\n",
    "space = dict()\n",
    "cross_folding(GaussianNB(), space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second graph represents the Precision-Recall curve. The curve shows how the precision of the classifier varies as the recall threshold changes. As the recall threshold increases, the classifier captures more positive cases, resulting in higher recall.\n",
    "\n",
    "The ideal scenario is to have both high precision and high recall, represented by a curve that hugs the top-right corner of the graph. A higher area under the Precision-Recall curve indicates a better-performing classifier with a good balance between precision and recall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "The SVM (Support Vector Machine) algorithm is a powerful and versatile supervised learning algorithm used for classification and regression tasks. It finds an optimal hyperplane that maximally separates the data points of different classes in the feature space.\n",
    "\n",
    "However, SVM may not perform well on certain datasets due to the following reasons:\n",
    "\n",
    "<strong>Sensitive to Noise and Outliers</strong>: SVM aims to find a decision boundary with maximum margin, making it sensitive to noisy or outlier data points. Outliers can disproportionately influence the position of the hyperplane and lead to suboptimal classification results.\n",
    "\n",
    "<strong>Lack of Probabilistic Interpretation</strong>: SVM originally provides binary classification and does not inherently provide probabilistic outputs. Additional techniques like Platt scaling or modified algorithms are required to estimate probabilities, making it less straightforward for tasks that require probability estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def support_vector(X_train, y_train, X_test, y_test):\n",
    "    # Create an instance of the SVM model\n",
    "    model = svm.SVC(kernel='linear')\n",
    "\n",
    "    # Train the model using the training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.imshow(cm, cmap='Blues', interpolation='None')\n",
    "    plt.xticks(np.arange(7))\n",
    "    plt.yticks(np.arange(7))\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "\n",
    "    # Add labels to the plot\n",
    "    width, height = cm.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "\n",
    "    # Define the class labels\n",
    "    labels = ['underweight', 'normal weight', 'overweight level I', 'overweight level II', 'obesity grade I', 'obesity grade II', 'obesity grade III']\n",
    "\n",
    "    # Get the count of predicted and actual labels\n",
    "    predicted_counts = [np.sum(y_pred == i) for i in np.unique(y_test)]\n",
    "    actual_counts = [np.sum(y_test == i) for i in np.unique(y_test)]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, predicted_counts, width, label='Predicted')\n",
    "    rects2 = ax.bar(x + width/2, actual_counts, width, label='Actual')\n",
    "\n",
    "    # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Actual vs Predicted Labels')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy \n",
    "\n",
    "\n",
    "print(\"--------------------- DATASET WITH ALL COLUMNS --------------------------\\n\")\n",
    "svm_accuracy = support_vector(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE AND MTRANS -------------------------------------\")\n",
    "svm_accuracy1 = support_vector(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC AND FAVC -------------------------------------\")\n",
    "svm_accuracy2 = support_vector(X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC, FAVC, family_history_with_overweight AND CALC -------------------------------------\")\n",
    "svm_accuracy3 = support_vector(X_train3, y_train3, X_test3, y_test3)\n",
    "\n",
    "print(\"--------------------- Grid Search with cross validation --------------------------\\n\")\n",
    "\n",
    "space = dict()\n",
    "cross_folding(svm.SVC(kernel='linear'), space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network üß†\n",
    "\n",
    "Neural networks, specifically deep learning algorithms, are a class of machine learning models inspired by the structure and function of the human brain. They consist of interconnected nodes or \"neurons\" organized into layers, enabling them to learn complex patterns and representations from data.\n",
    "\n",
    "<strong>Difficulty in Hyperparameter Tuning</strong>: Neural networks have numerous hyperparameters, such as the number of layers, the number of neurons per layer, activation functions, and learning rates. Selecting appropriate values for these hyperparameters requires expertise and extensive experimentation, which can be time-consuming and challenging.\n",
    "\n",
    "<strong>Sensitive to Data Quality and Preprocessing</strong>: Neural networks are highly sensitive to the quality and preprocessing of the input data. Noisy or inconsistent data, outliers, or missing values can adversely affect their performance. Proper data preprocessing and cleaning are crucial for obtaining reliable results.\n",
    "\n",
    "<strong>Prone to Overfitting</strong>: Neural networks, especially complex ones, are prone to overfitting if not properly regularized. Regularization techniques like dropout, weight decay, or early stopping are often employed to prevent overfitting and improve generalization.\n",
    "\n",
    "<strong>Black Box Nature</strong>: Neural networks are often considered as \"black box\" models, meaning they lack interpretability compared to simpler algorithms. Understanding the inner workings and reasoning behind their predictions can be challenging, limiting their applicability in domains where interpretability is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(X_train, y_train, X_test, y_test):\n",
    "    # Generate or load your training data (X_train, y_train)\n",
    "    # Make sure the shape of X_train and y_train is appropriate\n",
    "\n",
    "    # Determine the number of input features\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # Determine the number of classes\n",
    "    num_classes = 7\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_shape=(input_dim,), activation='relu'))\n",
    "    model.add(Dense(75, activation='relu'))\n",
    "    #To prevent overfitting, we add a dropout layer\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Convert the target data to one-hot encoded format\n",
    "    y_train_encoded = np.eye(num_classes)[y_train]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_encoded, verbose=1, epochs=10)\n",
    "\n",
    "    # Perform predictions on test data\n",
    "    y_pred_encoded = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_encoded, axis=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "    return accuracy \n",
    "\n",
    "print(\"--------------------- DATASET WITH ALL COLUMNS --------------------------\\n\")\n",
    "nn_accuracy = neural_network(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE AND MTRANS -------------------------------------\")\n",
    "nn_accuracy1 = neural_network(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC AND FAVC -------------------------------------\")\n",
    "nn_accuracy2 = neural_network(X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC, FAVC, family_history_with_overweight AND CALC -------------------------------------\")\n",
    "nn_accuracy3 = neural_network(X_train3, y_train3, X_test3, y_test3)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbours üè°üè†\n",
    "\n",
    "The k-Nearest Neighbors (k-NN) algorithm is a non-parametric, instance-based supervised learning algorithm. It classifies a new data point by comparing it to the k nearest neighbors in the training dataset based on a distance metric (e.g., Euclidean distance).\n",
    "\n",
    "<strong>Sensitive to Irrelevant Features</strong>: k-NN considers all features equally important when calculating distances. If the dataset contains irrelevant features or features with different scales, it can negatively impact the accuracy of the algorithm. Feature selection or dimensionality reduction techniques may be required to mitigate this issue.\n",
    "\n",
    "<strong>Imbalanced Data</strong>: In datasets with imbalanced class distributions, k-NN tends to favor the majority class due to the voting mechanism. This can lead to biased predictions and poor performance on the minority class. Resampling techniques or adjusting the voting mechanism can help alleviate this problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Create an instance of the KNN model\n",
    "\n",
    "def k_nearest_neighbors(X_train, y_train, X_test, y_test):\n",
    "    model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "    # Train the model using the training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "    # Visualize accuracy\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.barh([0], [accuracy], color='lightblue')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    plt.gca().tick_params(left=False, labelleft=False)\n",
    "    plt.gca().tick_params(axis='x', direction='out', length=6, width=2, color='gray')\n",
    "    plt.gca().tick_params(axis='x', which='minor', bottom=False, labelbottom=False)\n",
    "    plt.grid(axis='x', color='gray', linestyle='--')\n",
    "\n",
    "    # Add numerical value annotation\n",
    "    plt.text(accuracy + 0.01, 0, '{:.2%}'.format(accuracy), va='center')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot the classification report as a heatmap\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    report_lines = report.split(\"\\n\")[2:-5]\n",
    "    labels = []\n",
    "    data = []\n",
    "    for line in report_lines:\n",
    "        row = line.split()\n",
    "        if len(row) > 1:\n",
    "            labels.append(row[0])\n",
    "            values = [float(x) for x in row[1: len(row) - 1]]\n",
    "            data.append(values)\n",
    "    data = np.array(data)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    #sns.heatmap(data, annot=True, cmap='Blues', cbar=False, xticklabels=[\"Precision\", \"Recall\", \"F1-score\"], yticklabels=labels)\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Class')\n",
    "    plt.title('Classification Report')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "print(\"--------------------- DATASET WITH ALL COLUMNS --------------------------\\n\")\n",
    "k_nearest_neighbors_accuracy = k_nearest_neighbors(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE AND MTRANS -------------------------------------\")\n",
    "k_nearest_neighbors_accuracy1 = k_nearest_neighbors(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC AND FAVC -------------------------------------\")\n",
    "k_nearest_neighbors_accuracy2 = k_nearest_neighbors(X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "print(\"------------------------------ DATASET WITHOUT SCC, SMOKE, MTRANS, CAEC, FAVC, family_history_with_overweight AND CALC -------------------------------------\")\n",
    "k_nearest_neighbors_accuracy3 =k_nearest_neighbors(X_train3, y_train3, X_test3, y_test3)\n",
    "\n",
    "print(\"--------------------- Grid Search with cross validation --------------------------\\n\")\n",
    "\n",
    "space = dict()\n",
    "cross_folding(KNeighborsClassifier(n_neighbors=3), space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_values = [decision_tree_accuracy, nayve_bayes_accuracy, svm_accuracy, nn_accuracy,k_nearest_neighbors_accuracy]\n",
    "data1_values = [decision_tree_accuracy1, nayve_bayes_accuracy1, svm_accuracy1, nn_accuracy1, k_nearest_neighbors_accuracy1]\n",
    "data2_values = [decision_tree_accuracy2, nayve_bayes_accuracy2, svm_accuracy2, nn_accuracy2, k_nearest_neighbors_accuracy2]\n",
    "data3_values = [decision_tree_accuracy3, nayve_bayes_accuracy3, svm_accuracy3, nn_accuracy3, k_nearest_neighbors_accuracy3]\n",
    "\n",
    "index = ['Decision Tree', 'Nayve Bayes', 'SVM', 'Neural Network', 'KNN']\n",
    "df = pd.DataFrame({'Original Data': data_values,\n",
    "                    'Data 1': data1_values,\n",
    "                    'Data 2': data2_values,\n",
    "                    'Data 3': data3_values,\n",
    "                    }, index=index)\n",
    "ax = df.plot.bar(rot=0)\n",
    "\n",
    "for bar in ax.patches:\n",
    "  # The text annotation for each bar should be its height.\n",
    "  bar_value = bar.get_height()\n",
    "  # Format the text with commas to separate thousands. You can do\n",
    "  # any type of formatting here though.\n",
    "  text = f'{bar_value:.1%}'\n",
    "  #{\"Accuracy: {:.2%}\".format(accuracy)\n",
    "  # This will give the middle of each bar on the x-axis.\n",
    "  text_x = bar.get_x() + bar.get_width() / 2\n",
    "  # get_y() is where the bar starts so we add the height to it.\n",
    "  text_y = bar.get_y() + bar_value\n",
    "  # If we want the text to be the same color as the bar, we can\n",
    "  # get the color like so:\n",
    "  bar_color = bar.get_facecolor()\n",
    "  # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "          size=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "- [Scikit-Learn](https://scikit-learn.org/)\n",
    "- [Seaborn](https://seaborn.pydata.org/)\n",
    "- [Matplotlib](https://matplotlib.org/)\n",
    "- [Pandas](https://pandas.pydata.org/)\n",
    "- [Imblearn](https://imbalanced-learn.org/stable/)\n",
    "\n",
    "#### Articles\n",
    "\n",
    "- J. Pajuelo Ram√≠rez, L. Torres Aparcana, R. Ag√ºero Zamora, y I. Bernui Leo\n",
    "El sobrepeso, la obesidad y la obesidad abdominal en la poblaci√≥n adulta del Per√∫\n",
    "An Fac Med, 80 (1) (2019)\n",
    "\n",
    "- T. W Elffers, R. de Mutsert, H. J Lamb, A. de Roo, K.W. van Dijk, F. R Rosendaal, J. Wouter Jukema, y S. Trompet\n",
    "Body fat distribution, in particular visceral fat, is associated with cardiometabolic risk factors in obese women\n",
    "PLoS One (2017)\n",
    "\n",
    "- M. Safaei, E.A. Sundararajan, W. Boulila, y A. Shapi'i\n",
    "A systematic literature review on obesity: understanding the causes & consequences of obesity and reviewing various machine learning approaches used to predict obesity\n",
    "Comput Biol Med, 136 (104754) (2021)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
